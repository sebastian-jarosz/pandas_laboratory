{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Uczenie Maszynowe Lab Cw1_Cw2 Jarosz.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDGpFNgwPYvz6U2YjJW2ME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastian-jarosz/pandas_laboratory/blob/main/Uczenie_Maszynowe_Lab_Cw1_Cw2_Jarosz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jdzzxHA9kti",
        "outputId": "f98ed867-0df0-420d-a1d5-5a205f161575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.47207278]\n",
            " [0.5481679 ]\n",
            " [0.42728978]\n",
            " [0.4668929 ]]\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.Variable\n",
        "\n",
        "m = Sequential()\n",
        "m.add(Dense(2,input_dim=2, activation='tanh'))\n",
        "m.add(Dense(12,input_dim=2, activation='tanh'))\n",
        "#m.add(Activation('tanh'))\n",
        "\n",
        "m.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#m.add(Activation('sigmoid'))\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]],'float32')\n",
        "Y = np.array([[0],[1],[1],[0]],'float32')\n",
        "\n",
        "m.compile(optimizer='adam',loss='binary_crossentropy')\n",
        "m.fit(X,Y,batch_size=1,epochs=200,verbose=0)\n",
        "print(m.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X1=tf.Variable(initial_value=[0.,1.,0.,1.])\n",
        "X2=tf.Variable(initial_value=[0.,0.,1.,1.])\n",
        "Y_AND=tf.Variable(initial_value=[0.,0.,0.,1.])\n",
        "Y_NOR=tf.Variable(initial_value=[1.,0.,0.,0.])\n",
        "Y_OR=tf.Variable(initial_value=[0.,1.,1.,1.])\n",
        "Y_XOR=tf.Variable(initial_value=[0.,1.,1.,0.])\n",
        "Y_XNOR=tf.Variable(initial_value=[1.,0.,0.,1.])\n",
        "\n",
        "class Model(object):\n",
        " \n",
        "   def __init__(self):\n",
        "     # Initialize the weights to `2.0` and the bias to `2.0`\n",
        "     # In practice, these should be initialized to random values\n",
        "     # self.W1=tf.random.normal([1])\n",
        "     self.W1 = tf.Variable(2.)\n",
        "     self.W2= tf.Variable(2.)\n",
        "     self.b=tf.Variable(2.)\n",
        "   \n",
        "   def __call__(self, x1,x2):\n",
        "     self.K1=self.W1 * x1 + self.W2 * x2 + self.b\n",
        "     self.Output = tf.keras.activations.sigmoid(self.K1)\n",
        "     return(self.Output)\n",
        "model=Model()\n",
        "\n",
        "def compute_cost(target_y, predicted_y):\n",
        "    c=tf.keras.losses.BinaryCrossentropy()\n",
        "    return(c(target_y,predicted_y))\n",
        "\n",
        "def None_to_Zero(v):\n",
        "   if v==None:\n",
        "      v=0\n",
        "   return v\n",
        "\n",
        "def train(model, X1, X2, Y2, learning_rate):\n",
        "     with tf.GradientTape() as t:\n",
        "          current_loss = compute_cost(Y2,model(X1,X2))\n",
        "     dW1,dW2,db=t.gradient(current_loss,[model.W1,model.W2,model.b])          \n",
        "     dW1=None_to_Zero(dW1)\n",
        "     db=None_to_Zero(db)\n",
        "     dW2=None_to_Zero(dW2)\n",
        "     model.W1.assign_sub(learning_rate * dW1)\n",
        "     model.b.assign_sub(learning_rate * db)\n",
        "     model.W2.assign_sub(learning_rate * dW2)\n",
        "\n",
        "# OR\n",
        "# As we have very less Input data we need to increase the number of # epochs.\n",
        "epochs=range(1000)\n",
        "learning_rate=0.1\n",
        "for epoch in epochs:\n",
        "   train(model, X1, X2,Y_OR, learning_rate)\n",
        "#Once the training is done we can find results by calling the Model  #Class object as \n",
        "print(np.round(model(X1, X2).numpy()))\n",
        "\n",
        "W_OR={'W1': model.W1.numpy(), 'W2': model.W2.numpy(), 'b': model.b.numpy()}\n",
        "\n",
        "# AND\n",
        "# As we have very less Input data we need to increase the number of # epochs.\n",
        "epochs=range(1000)\n",
        "learning_rate=0.1\n",
        "for epoch in epochs:\n",
        "   train(model, X1, X2,Y_AND, learning_rate)\n",
        "#Once the training is done we can find results by calling the Model  #Class object as \n",
        "print(np.round(model(X1, X2).numpy()))\n",
        "\n",
        "W_AND={'W1': model.W1.numpy(), 'W2': model.W2.numpy(), 'b': model.b.numpy()}\n",
        "\n",
        "# NOR\n",
        "# As we have very less Input data we need to increase the number of # epochs.\n",
        "epochs=range(1000)\n",
        "learning_rate=0.1\n",
        "for epoch in epochs:\n",
        "   train(model, X1, X2,Y_NOR, learning_rate)\n",
        "#Once the training is done we can find results by calling the Model  #Class object as \n",
        "print(np.round(model(X1, X2).numpy()))\n",
        "\n",
        "W_NOR={'W1': model.W1.numpy(), 'W2': model.W2.numpy(), 'b': model.b.numpy()}\n",
        "\n",
        "# XOR\n",
        "class Model_XOR(object):\n",
        " \n",
        "     def __init__(self):\n",
        "          self.W11 = tf.Variable(W_AND['W1'])\n",
        "          self.W12= tf.Variable(W_AND['W2'])\n",
        "          self.b13=tf.Variable(W_AND['b'])\n",
        "          self.W14 = tf.Variable(W_NOR['W1'])\n",
        "          self.W15= tf.Variable(W_NOR['W2'])\n",
        "          self.b16=tf.Variable(W_NOR['b'])\n",
        "          self.W21 = tf.Variable(W_NOR['W1'])\n",
        "          self.W22= tf.Variable(W_NOR['W2'])\n",
        "          self.b23=tf.Variable(W_NOR['b'])\n",
        "\n",
        "     def __call__(self, x1,x2):\n",
        "          self.W1=self.W11 * x1 + self.W12*x2 + self.b13\n",
        "          self.W2=self.W14 * x1 + self.W15*x2 + self.b16\n",
        "          self.A1 = tf.keras.activations.sigmoid(self.W1)\n",
        "          self.A2 = tf.keras.activations.sigmoid(self.W2)\n",
        "          self.W3=self.W21 * self.A1 + self.W22*self.A2 + self.b23\n",
        "          self.A3 = tf.keras.activations.sigmoid(self.W3)\n",
        "          return(self.A3)\n",
        "model_xor=Model_XOR()\n",
        "\n",
        "\n",
        "def train_xor(model_xor, X1, X2, Y2, learning_rate):\n",
        "      with tf.GradientTape() as t:\n",
        "           current_loss = compute_cost(Y2,model_xor(X1,X2))\n",
        " \n",
        "      dW11,dW12,db13,dW14,dW15,db16,dW21,dW22,db23=t.gradient(current_loss,\n",
        "                                                              [model_xor.W11,model_xor.W12,model_xor.b13,\n",
        "                                                               model_xor.W14,model_xor.W15,model_xor.b16,\n",
        "                                                               model_xor.W21,model_xor.W22,model_xor.b23,])\n",
        "                                                          \n",
        "      model_xor.W11.assign_sub(learning_rate * None_to_Zero(dW11))\n",
        "      model_xor.b13.assign_sub(learning_rate * None_to_Zero(db13))\n",
        "      model_xor.W12.assign_sub(learning_rate * None_to_Zero(dW12))\n",
        "      model_xor.W14.assign_sub(learning_rate * None_to_Zero(dW14))\n",
        "      model_xor.b16.assign_sub(learning_rate * None_to_Zero(db16))\n",
        "      model_xor.W15.assign_sub(learning_rate * None_to_Zero(dW15))\n",
        "      model_xor.W21.assign_sub(learning_rate * None_to_Zero(dW21))\n",
        "      model_xor.b23.assign_sub(learning_rate * None_to_Zero(db23))\n",
        "      model_xor.W22.assign_sub(learning_rate * None_to_Zero(dW22))\n",
        "\n",
        "epochs=range(100)\n",
        "learning_rate=0.1\n",
        "for epoch in epochs:\n",
        "   train_xor(model_xor, X1, X2,Y_XOR, learning_rate)\n",
        "   #Once the training is done we can find results by calling the Model_XOR  #Class object as \n",
        "print(np.round(model_xor(X1, X2).numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z56YB5LJEFk7",
        "outputId": "517119a3-0c62-4346-9b1d-64fc572e0b48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 1. 1.]\n",
            "[0. 0. 0. 1.]\n",
            "[1. 0. 0. 0.]\n",
            "[0. 1. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.Variable\n",
        "\n",
        "m = Sequential()\n",
        "m.add(Dense(2,input_dim=2, activation='tanh'))\n",
        "m.add(Dense(12,input_dim=2, activation='tanh'))\n",
        "#m.add(Activation('tanh'))\n",
        "\n",
        "m.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#m.add(Activation('sigmoid'))\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1],[1,0],[0,1],[1,1],[0,0]],'float32')\n",
        "Y = np.array([[0],[1],[1],[0],[1],[0],[1],[1]],'float32')\n",
        "\n",
        "m.compile(optimizer='adam',loss='binary_crossentropy')\n",
        "m.fit(X,Y,batch_size=1,epochs=200,verbose=0)\n",
        "print(m.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoHtE_4MLUAn",
        "outputId": "2a4dbe4f-696b-49b9-eec2-d0fc75042287"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.555702  ]\n",
            " [0.46640384]\n",
            " [0.8229134 ]\n",
            " [0.6702832 ]\n",
            " [0.8229134 ]\n",
            " [0.46640384]\n",
            " [0.6702832 ]\n",
            " [0.555702  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.Variable\n",
        "\n",
        "m = Sequential()\n",
        "m.add(Dense(2,input_dim=2, activation='tanh'))\n",
        "m.add(Dense(12,input_dim=2, activation='tanh'))\n",
        "#m.add(Activation('tanh'))\n",
        "\n",
        "m.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#m.add(Activation('sigmoid'))\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1],[1,0],[0,1]],'float32')\n",
        "Y = np.array([[0],[1],[1],[0],[1],[0]],'float32')\n",
        "\n",
        "m.compile(optimizer='adam',loss='binary_crossentropy')\n",
        "m.fit(X,Y,batch_size=1,epochs=200,verbose=0)\n",
        "print(m.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiavLb36Li4U",
        "outputId": "24cd4b43-773c-4160-ca58-77726d499472"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.43738803]\n",
            " [0.2332676 ]\n",
            " [0.7892575 ]\n",
            " [0.37953925]\n",
            " [0.7892575 ]\n",
            " [0.2332676 ]]\n"
          ]
        }
      ]
    }
  ]
}